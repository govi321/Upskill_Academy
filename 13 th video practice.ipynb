{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f660c3-d522-417d-8131-bb1198e98185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2365)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1068)\n",
       "\tat scala.collection.immutable.List.flatMap(List.scala:366)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1049)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:679)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:497)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:224)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:87)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:54)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)\n",
       "\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\n",
       "\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\n",
       "\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\n",
       "\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\n",
       "\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1068)\n\tat scala.collection.immutable.List.flatMap(List.scala:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1049)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:679)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:497)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:87)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:56)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:56)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:54)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:88)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\n\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)</div>",
       "errorSummary": "AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/rawdata/sales.csv. SQLSTATE: 42K03",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val df = spark.read.option(\"header\", \"true\").csv(\"/mnt/rawdata/sales.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f225cb45-303b-4e0b-b0e5-ff0714377edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 30"
     ]
    }
   ],
   "source": [
    "%r\n",
    "# Create a simple vector and calculate mean\n",
    "numbers <- c(10, 20, 30, 40, 50)\n",
    "mean(numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f3d02c-dc1d-451d-b768-f1ab1472c6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### This is my note book\n",
    "**this my line**\n",
    ". one more text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6080b17-dbcf-4589-86cd-20ccb105af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#this is\n",
    "\n",
    "![](https://cognitivetoday.com/wp-content/uploads/2012/08/Big-data-Different.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0cb9aca-d101-4e0c-ae66-ab46992fb45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%run\n",
    "##### I want to run the 1.Notebook code to 2.Note book code we use % run  \n",
    "\n",
    "%fs\n",
    "#### fs is the file system\n",
    "\n",
    "%pip\n",
    "### python installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10790e9-34cd-4360-b270-ae0d9b9de5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Data bricks widgets\n",
    ".Textbox widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e8f744-1b24-44b0-8535-cd4a7341c2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(name='FruitText', defaultValue='Apple', label='TextBox')\n",
    "value = dbutils.widgets.get('FruitText')\n",
    "print(\"value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5e69f6-7296-4a9a-8fd7-eefd102f02d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Combobox`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559beb69-c3f7-495d-bac8-094f08996bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.combobox(name=\"Fruitecombobox\", defaultValue=\"Apple\", choices=[\"Apple\", \"Orange\", \"Banana\", \"Pear\"], label=\"combo\")\n",
    "\n",
    "value = dbutils.widgets.get(\"Fruitecombobox\")\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e34a4b93-6186-44d4-8a97-273247e617cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3380b4-5d2f-45a8-b7e7-263612311c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.widgets.dropdown(name=\"Fruitedropdown\", defaultValue=\"Apple\", choices=[\"Apple\", \"Orange\", \"Banana\", \"Pear\"], label=\"dropdown\")\n",
    "\n",
    "value = dbutils.widgets.get(\"Fruitedropdown\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65872319-8e97-4fc0-84f5-5ac45a6b3458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Multiselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da4612d-a4ba-4dfb-8c81-7622968d51bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.multiselect(name=\"Fruitemultiselect\", defaultValue=\"Apple\", choices=[\"Apple\", \"Orange\", \"Banana\", \"Pear\"], label=\"multiselect\")\n",
    "\n",
    "value = dbutils.widgets.get(\"Fruitemultiselect\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9066dcba-d9dc-4bf5-9b4a-2cc25fb333d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample widget file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77dce65-54f0-4298-8626-d8dfc0019d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderID</th><th>CustomerID</th><th>OrderDate</th><th>Country</th><th>Product</th><th>Quantity</th><th>Price</th><th>Department</th></tr></thead><tbody><tr><td>1001</td><td>C001</td><td>2022-01-15</td><td>USA</td><td>Laptop</td><td>1</td><td>1000.0</td><td>Electronics</td></tr><tr><td>1002</td><td>C002</td><td>2022-02-10</td><td>India</td><td>Mouse</td><td>2</td><td>25.0</td><td>Electronics</td></tr><tr><td>1003</td><td>C003</td><td>2022-03-18</td><td>UK</td><td>Keyboard</td><td>1</td><td>45.0</td><td>Electronics</td></tr><tr><td>1004</td><td>C004</td><td>2022-04-20</td><td>USA</td><td>Desk</td><td>1</td><td>150.0</td><td>Furniture</td></tr><tr><td>1005</td><td>C005</td><td>2022-05-10</td><td>India</td><td>Chair</td><td>4</td><td>75.0</td><td>Furniture</td></tr><tr><td>1006</td><td>C006</td><td>2022-06-30</td><td>Germany</td><td>Laptop</td><td>1</td><td>1200.0</td><td>Electronics</td></tr><tr><td>1007</td><td>C007</td><td>2022-07-25</td><td>UK</td><td>Monitor</td><td>2</td><td>300.0</td><td>Electronics</td></tr><tr><td>1008</td><td>C008</td><td>2022-10-12</td><td>USA</td><td>Table</td><td>1</td><td>200.0</td><td>Furniture</td></tr><tr><td>1009</td><td>C009</td><td>2023-01-20</td><td>India</td><td>Notebook</td><td>10</td><td>5.0</td><td>Stationery</td></tr><tr><td>1010</td><td>C010</td><td>2023-03-15</td><td>France</td><td>Pen</td><td>20</td><td>1.0</td><td>Stationery</td></tr><tr><td>1011</td><td>C011</td><td>2023-04-05</td><td>USA</td><td>Mousepad</td><td>5</td><td>10.0</td><td>Electronics</td></tr><tr><td>1012</td><td>C012</td><td>2023-05-10</td><td>Germany</td><td>Printer</td><td>1</td><td>250.0</td><td>Electronics</td></tr><tr><td>1013</td><td>C013</td><td>2023-06-10</td><td>India</td><td>Laptop</td><td>1</td><td>1050.0</td><td>Electronics</td></tr><tr><td>1014</td><td>C014</td><td>2023-07-21</td><td>UK</td><td>Chair</td><td>2</td><td>80.0</td><td>Furniture</td></tr><tr><td>1015</td><td>C015</td><td>2023-08-30</td><td>France</td><td>Mouse</td><td>1</td><td>20.0</td><td>Electronics</td></tr><tr><td>1016</td><td>C016</td><td>2024-01-09</td><td>USA</td><td>Keyboard</td><td>2</td><td>50.0</td><td>Electronics</td></tr><tr><td>1017</td><td>C017</td><td>2024-02-28</td><td>India</td><td>Desk</td><td>1</td><td>175.0</td><td>Furniture</td></tr><tr><td>1018</td><td>C018</td><td>2025-03-25</td><td>UK</td><td>Table</td><td>1</td><td>220.0</td><td>Furniture</td></tr><tr><td>1019</td><td>C019</td><td>2025-04-19</td><td>Germany</td><td>Pen</td><td>15</td><td>1.5</td><td>Stationery</td></tr><tr><td>1020</td><td>C020</td><td>2025-05-03</td><td>USA</td><td>Notebook</td><td>5</td><td>4.5</td><td>Stationery</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1001,
         "C001",
         "2022-01-15",
         "USA",
         "Laptop",
         1,
         1000.0,
         "Electronics"
        ],
        [
         1002,
         "C002",
         "2022-02-10",
         "India",
         "Mouse",
         2,
         25.0,
         "Electronics"
        ],
        [
         1003,
         "C003",
         "2022-03-18",
         "UK",
         "Keyboard",
         1,
         45.0,
         "Electronics"
        ],
        [
         1004,
         "C004",
         "2022-04-20",
         "USA",
         "Desk",
         1,
         150.0,
         "Furniture"
        ],
        [
         1005,
         "C005",
         "2022-05-10",
         "India",
         "Chair",
         4,
         75.0,
         "Furniture"
        ],
        [
         1006,
         "C006",
         "2022-06-30",
         "Germany",
         "Laptop",
         1,
         1200.0,
         "Electronics"
        ],
        [
         1007,
         "C007",
         "2022-07-25",
         "UK",
         "Monitor",
         2,
         300.0,
         "Electronics"
        ],
        [
         1008,
         "C008",
         "2022-10-12",
         "USA",
         "Table",
         1,
         200.0,
         "Furniture"
        ],
        [
         1009,
         "C009",
         "2023-01-20",
         "India",
         "Notebook",
         10,
         5.0,
         "Stationery"
        ],
        [
         1010,
         "C010",
         "2023-03-15",
         "France",
         "Pen",
         20,
         1.0,
         "Stationery"
        ],
        [
         1011,
         "C011",
         "2023-04-05",
         "USA",
         "Mousepad",
         5,
         10.0,
         "Electronics"
        ],
        [
         1012,
         "C012",
         "2023-05-10",
         "Germany",
         "Printer",
         1,
         250.0,
         "Electronics"
        ],
        [
         1013,
         "C013",
         "2023-06-10",
         "India",
         "Laptop",
         1,
         1050.0,
         "Electronics"
        ],
        [
         1014,
         "C014",
         "2023-07-21",
         "UK",
         "Chair",
         2,
         80.0,
         "Furniture"
        ],
        [
         1015,
         "C015",
         "2023-08-30",
         "France",
         "Mouse",
         1,
         20.0,
         "Electronics"
        ],
        [
         1016,
         "C016",
         "2024-01-09",
         "USA",
         "Keyboard",
         2,
         50.0,
         "Electronics"
        ],
        [
         1017,
         "C017",
         "2024-02-28",
         "India",
         "Desk",
         1,
         175.0,
         "Furniture"
        ],
        [
         1018,
         "C018",
         "2025-03-25",
         "UK",
         "Table",
         1,
         220.0,
         "Furniture"
        ],
        [
         1019,
         "C019",
         "2025-04-19",
         "Germany",
         "Pen",
         15,
         1.5,
         "Stationery"
        ],
        [
         1020,
         "C020",
         "2025-05-03",
         "USA",
         "Notebook",
         5,
         4.5,
         "Stationery"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CustomerID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "OrderDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_widget = spark.read.option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .csv('dbfs:/FileStore/tables/widgets.csv')\n",
    "\n",
    "display(df_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1ea714-502c-40ab-8c62-e90ccb608b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderID</th><th>CustomerID</th><th>OrderDate</th><th>Country</th><th>Product</th><th>Quantity</th><th>Price</th><th>Department</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CustomerID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "OrderDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, lit\n",
    "\n",
    "# Step 1: Create widgets\n",
    "dbutils.widgets.text(\"OrderDate\", \"\", \"Order Date (YYYY-MM-DD)\")\n",
    "dbutils.widgets.dropdown(\"Country\", \"India\", [\"USA\", \"India\", \"UK\", \"Germany\", \"France\"])\n",
    "dbutils.widgets.combobox(\"Product\", \"Laptop\", [\"Laptop\", \"Mouse\", \"Keyboard\", \"Desk\", \"Chair\", \"Monitor\", \"Table\", \"Notebook\", \"Pen\", \"Mousepad\", \"Printer\"])\n",
    "dbutils.widgets.multiselect(\"Department\", \"Electronics\", [\"Electronics\", \"Furniture\", \"Stationery\"])\n",
    "\n",
    "# Step 2: Read widget values\n",
    "order_date = dbutils.widgets.get(\"OrderDate\")\n",
    "country = dbutils.widgets.get(\"Country\")\n",
    "product = dbutils.widgets.get(\"Product\")\n",
    "departments = dbutils.widgets.get(\"Department\").split(\",\")\n",
    "\n",
    "# Step 3: Start filtering\n",
    "# make sure df_widget is your actual DataFrame name\n",
    "\n",
    "filtered_df = df_widget\n",
    "\n",
    "if country:\n",
    "    filtered_df = filtered_df.filter(col(\"Country\") == country)\n",
    "\n",
    "if product:\n",
    "    filtered_df = filtered_df.filter(col(\"Product\") == product)\n",
    "\n",
    "if order_date:\n",
    "    filtered_df = filtered_df.withColumn(\"OrderDate\", to_date(\"OrderDate\")) \\\n",
    "                             .filter(col(\"OrderDate\") >= to_date(lit(order_date)))\n",
    "if product:\n",
    "    filtered_df = filtered_df.filter(col(\"Product\") == product)\n",
    "\n",
    "if order_date:\n",
    "    filtered_df = filtered_df.withColumn(\"OrderDate\", to_date(\"OrderDate\")) \\\n",
    "                             .filter(col(\"OrderDate\") >= to_date(lit(order_date)))\n",
    "\n",
    "if departments and departments != ['']:\n",
    "    filtered_df = filtered_df.filter(col(\"Department\").isin(departments))\n",
    "\n",
    "# Step 4: Display result\n",
    "display(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd5d119-5174-44ad-8b1c-20b7771dbea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5222c7-fe09-4a4f-b180-2003e4946e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "This module provides various utilities for users to interact with the rest of Databricks.\n",
       "  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.help()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b40c16-8409-4cb3-a5f0-918d8ac48a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##File systemhelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e4c251-3dff-4c56-b771-e71017343115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cffe2c-c74e-460a-8568-ec2f723ccd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b397b1-04c1-41ce-a2af-d22b23b4ed38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[float, str, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n\n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n\n    .. versionadded:: 2.0.0\n\n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n\n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n\n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n        for the version you use.\n\n        .. # noqa\n\n    Examples\n    --------\n    Write a DataFrame into a CSV file and read it back.\n\n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory(prefix=\"csv\") as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 th video practice",
   "widgets": {
    "Country": {
     "currentValue": "UK",
     "nuid": "fe544f5e-171f-4809-bd7b-88287594445c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "India",
      "label": null,
      "name": "Country",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "USA",
        "India",
        "UK",
        "Germany",
        "France"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "India",
      "label": null,
      "name": "Country",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "USA",
        "India",
        "UK",
        "Germany",
        "France"
       ]
      }
     }
    },
    "Department": {
     "currentValue": "Furniture",
     "nuid": "b0cad603-3548-48d3-864d-df1fd23e2727",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Electronics",
      "label": null,
      "name": "Department",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Electronics",
        "Furniture",
        "Stationery"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "Electronics",
      "label": null,
      "name": "Department",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Electronics",
        "Furniture",
        "Stationery"
       ]
      }
     }
    },
    "FruitText": {
     "currentValue": "",
     "nuid": "56d09235-371d-49a0-a251-aef74728df23",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Apple",
      "label": "TextBox",
      "name": "FruitText",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Apple",
      "label": "TextBox",
      "name": "FruitText",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "Fruitecombobox": {
     "currentValue": "Apple",
     "nuid": "57b25ecb-3895-42c5-b038-f470747f73fd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Apple",
      "label": "combo",
      "name": "Fruitecombobox",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ],
       "fixedDomain": false,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "Apple",
      "label": "combo",
      "name": "Fruitecombobox",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ]
      }
     }
    },
    "Fruitedropdown": {
     "currentValue": "Apple",
     "nuid": "594b4430-0080-4643-a63f-3d4ea2a38724",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Apple",
      "label": "dropdown",
      "name": "Fruitedropdown",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Apple",
      "label": "dropdown",
      "name": "Fruitedropdown",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ]
      }
     }
    },
    "Fruitemultiselect": {
     "currentValue": "",
     "nuid": "e73e772c-fc09-4b09-ac64-8942ae04ad36",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Apple",
      "label": "multiselect",
      "name": "Fruitemultiselect",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "Apple",
      "label": "multiselect",
      "name": "Fruitemultiselect",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Apple",
        "Orange",
        "Banana",
        "Pear"
       ]
      }
     }
    },
    "OrderDate": {
     "currentValue": "",
     "nuid": "2d25cd76-29e1-4217-8eb6-21fa215e1785",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Order Date (YYYY-MM-DD)",
      "name": "OrderDate",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Order Date (YYYY-MM-DD)",
      "name": "OrderDate",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Product": {
     "currentValue": "Printer",
     "nuid": "99fcb5a0-57ed-4bda-9a6a-4e4e1779fa20",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Laptop",
      "label": null,
      "name": "Product",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Laptop",
        "Mouse",
        "Keyboard",
        "Desk",
        "Chair",
        "Monitor",
        "Table",
        "Notebook",
        "Pen",
        "Mousepad",
        "Printer"
       ],
       "fixedDomain": false,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "Laptop",
      "label": null,
      "name": "Product",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Laptop",
        "Mouse",
        "Keyboard",
        "Desk",
        "Chair",
        "Monitor",
        "Table",
        "Notebook",
        "Pen",
        "Mousepad",
        "Printer"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}